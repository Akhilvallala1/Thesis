{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilvallala2023/Thesis/blob/main/Light_eval_verification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ZBBxIDhW61"
      },
      "source": [
        "#Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8AyXH-oL8G1",
        "outputId": "139f027f-7d68-4dc0-819b-5cfe59a1f099"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "# Step 1: Install LightEval with Accelerate\n",
        "!pip install lighteval[accelerate]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RoFAkn9j4iP"
      },
      "source": [
        "#Testing on SmolLM-135M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfjKtZE6ioB1",
        "outputId": "4d1cb02e-b508-4e0e-8fd8-7bbfda89fbf7"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set up model and output directory\n",
        "MODEL = \"HuggingFaceTB/SmolLM-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Step 3: Run evaluation script for Hellaswag with SmolLM\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|hellaswag|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrnOofMEkllZ"
      },
      "source": [
        "#Testing both the models at same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfyEyDo5khW4",
        "outputId": "7b902ebe-3217-4ca2-df4c-f42974d01ce3"
      },
      "outputs": [],
      "source": [
        "# Set up the models and output directories\n",
        "MODELS = [\"gpt2\", \"HuggingFaceTB/SmolLM-135M\"]\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "for model in MODELS:\n",
        "    !lighteval accelerate \\\n",
        "        --model_args \"pretrained=$model\" \\\n",
        "        --tasks \"leaderboard|hellaswag|0|0\" \\\n",
        "        --override_batch_size 16 \\\n",
        "        --output_dir ${OUTPUT_DIR}/$model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMLN4BNfGKKa"
      },
      "source": [
        "#MMLU physics on smolLM models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFABseDoRako",
        "outputId": "781f79b6-41e7-42e1-b8dc-4d5d39ff95a0"
      },
      "outputs": [],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRrhnK0sReyJ",
        "outputId": "1f209ba2-d01e-4adc-e01a-01515ef309f0"
      },
      "outputs": [],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM-360M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9LzZgm5SHd1",
        "outputId": "4ac24706-412a-4836-ea88-fa754bd2a07e"
      },
      "outputs": [],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM-1.7B\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDBCTiRLVnNI",
        "outputId": "8f2707e4-84e1-4ce3-f014-1fa7c6dbb9fe"
      },
      "outputs": [],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idb7TCHChmQO"
      },
      "source": [
        "##MMLU physics on smolLM2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIFpfcJ3nclj",
        "outputId": "7611a428-e84a-4d7e-beb6-9026bfed0cf2"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set up model and output directory\n",
        "MODEL = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Step 3: Run evaluation script for Hellaswag with SmolLM\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihoBLiQfpdcY",
        "outputId": "cd2ccf22-16cc-4bb3-ec0e-53a189085661"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set up model and output directory\n",
        "MODEL = \"HuggingFaceTB/SmolLM2-360M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Step 3: Run evaluation script for Hellaswag with SmolLM\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE-sZJKfpi48",
        "outputId": "3184ada8-3056-4078-ba76-61b45535e7c9"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set up model and output directory\n",
        "MODEL = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Step 3: Run evaluation script for Hellaswag with SmolLM\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeqtIR5JnTle"
      },
      "source": [
        "#smolLM2 on Hellaswag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdQAtMNrX_SP",
        "outputId": "6c08fcce-2a24-4fb2-89e9-6e9763808283"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set up model and output directory\n",
        "MODEL = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Step 3: Run evaluation script for Hellaswag with SmolLM\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRL4k5EGppeF",
        "outputId": "93ded59e-0984-4570-a744-22671d13ed39"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set up model and output directory\n",
        "MODEL = \"HuggingFaceTB/SmolLM-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Step 3: Run evaluation script for Hellaswag with SmolLM\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|hellaswag|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfNXy9pKb_zw"
      },
      "source": [
        "#mmlu (cloze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5DSAWkwptgL",
        "outputId": "c5d530fc-7dbd-4021-aecc-aa06ce7066df"
      },
      "outputs": [],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"TIGER-Lab/MMLU-Pro\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc5od5fisoyJ"
      },
      "source": [
        "#Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "o7pvPxPvuwSY",
        "outputId": "5ac4a6b7-5f56-4f21-f53c-e20848d62f70"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Specify the path to your local model\n",
        "model_name = \"./fine_tuned_smolLM2_360M\"\n",
        "\n",
        "# Load the tokenizer and model from the local directory, enforcing local loading\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=True)\n",
        "\n",
        "# Save the model in PyTorch binary format if needed\n",
        "model.save_pretrained(model_name, safe_serialization=False)\n",
        "tokenizer.save_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8QQ0bpOcMnx",
        "outputId": "c5e45752-1c50-4b27-9577-476cdb305115"
      },
      "outputs": [],
      "source": [
        "# Set up model and output directory\n",
        "MODEL = \"./fine_tuned_smolLM2_360M\"  # Use your local model\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "# Run evaluation script with your saved model\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKJ18keqZ_GG",
        "outputId": "f06785f1-b681-41ed-b105-801074fb5ac4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaRWg85nq_Jv"
      },
      "source": [
        "#Evaluation on MMLU (cloze) cais/mmlu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1iOhj6StBN6",
        "outputId": "c83a8438-e099-4f2b-ef3d-23b82bbe7f20"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the SmolLM-135M model and tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Set the pad_token if it's not already set\n",
        "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
        "\n",
        "\n",
        "# Load the MMLU dataset with the \"all\" configuration to get all subjects\n",
        "dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"validation\")  # Use \"test\" for the test set\n",
        "\n",
        "# Evaluate function for a single question\n",
        "def evaluate_question(question, choices, correct_answer):\n",
        "    # Prepare the input in a cloze or multiple-choice prompt format\n",
        "    prompt = f\"Q: {question} Options: \" + \" \".join([f\"{chr(65 + i)}) {choice}\" for i, choice in enumerate(choices)])\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    # Get model logits for each choice\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]  # Get the logits for the last token\n",
        "\n",
        "    # Calculate scores for each choice by running the model on each option\n",
        "    choice_scores = []\n",
        "    for choice in choices:\n",
        "        choice_prompt = f\"{question} {choice}\"\n",
        "        choice_input_ids = tokenizer(choice_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            choice_output = model(choice_input_ids)\n",
        "        choice_score = choice_output.logits[:, -1, :].mean().item()\n",
        "        choice_scores.append(choice_score)\n",
        "\n",
        "    # Get the choice with the highest score\n",
        "    predicted_choice = torch.argmax(torch.tensor(choice_scores)).item()\n",
        "\n",
        "    # Check if the predicted choice matches the correct answer\n",
        "    is_correct = (predicted_choice == correct_answer)\n",
        "    return is_correct\n",
        "\n",
        "# Iterate through the dataset and evaluate each question\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for example in dataset:\n",
        "    question = example[\"question\"]\n",
        "    choices = example[\"choices\"]\n",
        "    correct_answer = example[\"answer\"]\n",
        "\n",
        "    is_correct = evaluate_question(question, choices, correct_answer)\n",
        "    if is_correct:\n",
        "        correct_count += 1\n",
        "    total_count += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_count / total_count\n",
        "print(f\"Accuracy on MMLU (All Subjects): {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzWbSOGitnh7"
      },
      "source": [
        "##Evaluation on MMLU (cloze) TIGER-Lab/MMLU-Pro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV9fMRfiZ9uj",
        "outputId": "bd11d9e7-588a-44a6-b4c4-49a95bc56ec3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the SmolLM-135M model and tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Set the pad_token if it's not already set\n",
        "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
        "\n",
        "# Load the MMLU dataset with the \"all\" configuration to get all subjects from TIGER-Lab/MMLU-Pro\n",
        "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\",split=\"validation\")  # Use \"test\" for the test set if needed\n",
        "\n",
        "# Evaluate function for a single question\n",
        "def evaluate_question(question, options, correct_answer_index):\n",
        "    # Prepare the input in a cloze or multiple-choice prompt format\n",
        "    prompt = f\"Q: {question} Options: \" + \" \".join([f\"{chr(65 + i)}) {option}\" for i, option in enumerate(options)])\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    # Get model logits for each choice\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]  # Get the logits for the last token\n",
        "\n",
        "    # Calculate scores for each option by running the model on each choice\n",
        "    choice_scores = []\n",
        "    for option in options:\n",
        "        choice_prompt = f\"{question} {option}\"\n",
        "        choice_input_ids = tokenizer(choice_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            choice_output = model(choice_input_ids)\n",
        "        choice_score = choice_output.logits[:, -1, :].mean().item()\n",
        "        choice_scores.append(choice_score)\n",
        "\n",
        "    # Get the choice with the highest score\n",
        "    predicted_choice_index = torch.argmax(torch.tensor(choice_scores)).item()\n",
        "\n",
        "    # Check if the predicted choice matches the correct answer\n",
        "    is_correct = (predicted_choice_index == correct_answer_index)\n",
        "    return is_correct\n",
        "\n",
        "# Iterate through the dataset and evaluate each question with tqdm progress bar\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for example in tqdm(dataset, desc=\"Evaluating questions\"):\n",
        "    question = example[\"question\"]\n",
        "    options = example[\"options\"]\n",
        "    correct_answer_index = example[\"answer_index\"]  # Use 'answer_index' for the correct option\n",
        "\n",
        "    is_correct = evaluate_question(question, options, correct_answer_index)\n",
        "    if is_correct:\n",
        "        correct_count += 1\n",
        "    total_count += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_count / total_count if total_count > 0 else 0\n",
        "print(f\"Accuracy on MMLU (All Subjects): {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UxGPFD8trWw",
        "outputId": "b0eb5c47-9d01-41bc-ee4b-d944dfda8bfc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the SmolLM-135M model and tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Set the pad_token if it's not already set\n",
        "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
        "\n",
        "# Load the MMLU dataset with the \"all\" configuration to get all subjects from TIGER-Lab/MMLU-Pro\n",
        "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\", split=\"validation\")  # Use \"test\" for the test set if needed\n",
        "\n",
        "# Evaluate function for a single question\n",
        "def evaluate_question(question, options, correct_answer_index):\n",
        "    # Prepare the input in a cloze or multiple-choice prompt format\n",
        "    prompt = f\"Q: {question} Options: \" + \" \".join([f\"{chr(65 + i)}) {option}\" for i, option in enumerate(options)])\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    # Get model logits for each choice\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]  # Get the logits for the last token\n",
        "\n",
        "    # Calculate scores for each option by running the model on each choice\n",
        "    choice_scores = []\n",
        "    for option in options:\n",
        "        choice_prompt = f\"{question} {option}\"\n",
        "        choice_input_ids = tokenizer(choice_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            choice_output = model(choice_input_ids)\n",
        "        choice_score = choice_output.logits[:, -1, :].mean().item()\n",
        "        choice_scores.append(choice_score)\n",
        "\n",
        "    # Get the choice with the highest score\n",
        "    predicted_choice_index = torch.argmax(torch.tensor(choice_scores)).item()\n",
        "\n",
        "    # Check if the predicted choice matches the correct answer\n",
        "    is_correct = (predicted_choice_index == correct_answer_index)\n",
        "    return is_correct\n",
        "\n",
        "# Iterate through the dataset and evaluate each question with tqdm progress bar\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for example in tqdm(dataset, desc=\"Evaluating questions\"):\n",
        "    question = example[\"question\"]\n",
        "    options = example[\"options\"]\n",
        "    correct_answer_index = example[\"answer_index\"]  # Use 'answer_index' for the correct option\n",
        "\n",
        "    # Evaluate the question and print debugging info if needed\n",
        "    is_correct = evaluate_question(question, options, correct_answer_index)\n",
        "    if is_correct:\n",
        "        correct_count += 1\n",
        "    total_count += 1\n",
        "\n",
        "    # Optional debugging output for the first few examples\n",
        "    if total_count <= 5:  # Adjust the number as needed\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Options: {options}\")\n",
        "        print(f\"Correct Answer Index: {correct_answer_index}\")\n",
        "        print(f\"Correct: {is_correct}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_count / total_count if total_count > 0 else 0\n",
        "print(f\"Correct Count: {correct_count}\")\n",
        "print(f\"Total Count: {total_count}\")\n",
        "print(f\"Accuracy on MMLU (All Subjects): {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWv2jeLlF3zI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPnE8hTK7akOjSEYOMrprOQ",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
