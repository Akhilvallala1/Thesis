{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilvallala2023/Thesis/blob/main/Fine_Tuning_using_LoRa_135M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "767fa4e7-de05-4fe0-9690-7f4c339bac26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "767fa4e7-de05-4fe0-9690-7f4c339bac26",
        "outputId": "f6b42a20-5305-4c29-ac60-9ff1ad195960"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8b3d3b2e193>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Prompt the user for their Hugging Face token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhuggingface_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your Hugging Face token: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Log in to Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Prompt the user for their Hugging Face token\n",
        "huggingface_token = input(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(token=huggingface_token)\n",
        "\n",
        "print(\"Successfully logged in to Hugging Face!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1cfd43-cd84-44bc-9268-09419b20599b",
      "metadata": {
        "id": "8b1cfd43-cd84-44bc-9268-09419b20599b",
        "outputId": "ae467b07-f9ab-48f4-9577-5ceeb443086e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample record from the dataset:\n",
            "{'role_1': 'Physicist_RoleType.ASSISTANT', 'topic;': 'Quantum mechanics', 'sub_topic': 'The Schrödinger equation and its solutions', 'message_1': 'What is the probability of finding a particle with a given energy in a one-dimensional infinite square well potential when the potential width is 2 nm and the particle has a mass of 5x10^-26 kg? Use the Schrödinger equation to solve for the allowed energy states and their wave functions.', 'message_2': 'To find the probability of finding a particle with a given energy in a one-dimensional infinite square well potential, we first need to solve the Schrödinger equation for the allowed energy states and their wave functions.\\n\\nThe time-independent Schrödinger equation for a one-dimensional infinite square well potential is given by:\\n\\n- (ħ^2 / 2m) * (d^2ψ(x) / dx^2) = E * ψ(x)\\n\\nwhere ħ is the reduced Planck constant (1.0545718 × 10^-34 Js), m is the mass of the particle (5 × 10^-26 kg), E is the energy of the particle, and ψ(x) is the wave function.\\n\\nThe boundary conditions for the infinite square well potential are:\\n\\nψ(0) = 0 and ψ(a) = 0\\n\\nwhere a is the width of the well (2 nm = 2 × 10^-9 m).\\n\\nThe general solution to the Schrödinger equation is:\\n\\nψ(x) = A * sin(kx) + B * cos(kx)\\n\\nwhere A and B are constants, and k = sqrt(2mE) / ħ.\\n\\nApplying the boundary conditions:\\n\\nψ(0) = A * sin(0) + B * cos(0) = B = 0\\nψ(a) = A * sin(ka) = 0\\n\\nSince A cannot be zero (otherwise the wave function would be trivial), sin(ka) must be zero. This occurs when ka = nπ, where n is an integer (1, 2, 3, ...). Therefore, the allowed wave numbers are:\\n\\nk_n = nπ / a\\n\\nThe allowed energy states are given by:\\n\\nE_n = (ħ^2 * k_n^2) / (2m) = (ħ^2 * n^2 * π^2) / (2m * a^2)\\n\\nThe corresponding wave functions are:\\n\\nψ_n(x) = A_n * sin(k_n * x)\\n\\nwhere A_n is a normalization constant.\\n\\nTo find the probability of finding a particle with a given energy E, we need to know the initial state of the particle. If the particle is in the nth energy state, the probability of finding it with energy E_n is 1. If the particle is in a superposition of energy states, we need to find the coefficients of the superposition and calculate the probability accordingly.\\n\\nIn summary, the allowed energy states and their wave functions for a particle in a one-dimensional infinite square well potential can be found using the Schrödinger equation. The probability of finding a particle with a given energy depends on the initial state of the particle and requires additional information.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face Hub\n",
        "dataset_name = \"akhilfau/physics_decontaminated_2\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")  # Adjust the split if needed (e.g., \"test\" or \"validation\")\n",
        "\n",
        "# Print a sample record\n",
        "print(\"Sample record from the dataset:\")\n",
        "print(dataset[0])  # Prints the first record from the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340e93b1-ff80-414c-a21c-8beb6fff9eab",
      "metadata": {
        "scrolled": true,
        "id": "340e93b1-ff80-414c-a21c-8beb6fff9eab",
        "outputId": "18ff86e5-9c78-4891-d82a-dc0ba9b84b5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 921,600 || all params: 135,436,608 || trainable%: 0.6805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32000' max='32000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32000/32000 1:54:51, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.014200</td>\n",
              "      <td>1.040344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.023100</td>\n",
              "      <td>1.008247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.999500</td>\n",
              "      <td>0.991766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.952700</td>\n",
              "      <td>0.982188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.935100</td>\n",
              "      <td>0.975225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.912600</td>\n",
              "      <td>0.971895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.916100</td>\n",
              "      <td>0.970611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.919400</td>\n",
              "      <td>0.970476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=32000, training_loss=0.974360009431839, metrics={'train_runtime': 6891.5252, 'train_samples_per_second': 18.574, 'train_steps_per_second': 4.643, 'total_flos': 4.2123286020096e+16, 'train_loss': 0.974360009431839, 'epoch': 8.0})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the Dataset\n",
        "dataset = load_dataset(\"akhilfau/physics_decontaminated_2\", split=\"train\")\n",
        "\n",
        "# Step 2: Load the Pretrained Model and Tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the padding token if not already set\n",
        "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
        "\n",
        "# Step 3: Configure LoRA with PEFT\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters to confirm LoRA is applied\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Step 4: Preprocess the Dataset\n",
        "def preprocess_function(examples):\n",
        "    # Concatenate the problem and solution for causal LM\n",
        "    inputs = [f\"Problem: {problem}\\nSolution: {solution}\" for problem, solution in zip(examples[\"message_1\"], examples[\"message_2\"])]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # Labels are the same as input_ids for causal LM\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Step 5: Define Training Arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    learning_rate=5e-4,  # Reduce the learning rate\n",
        "    lr_scheduler_type=\"cosine\",  # Use a more adaptive scheduler\n",
        "    per_device_train_batch_size=4,  # Increase if memory allows\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=8,  # Train for more epochs\n",
        "    weight_decay=0.1,  # Regularization\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    push_to_hub=False,\n",
        "    #gradient_checkpointing=True,  # Reduce memory usage if needed\n",
        "    bf16=False,  # Disable BFloat16\n",
        "    fp16=False,  # Disable FP16\n",
        ")\n",
        "\n",
        "# Step 6: Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Step 7: Train the Model\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab19d60-2a4b-4647-bdb6-f0f8c4aa2a27",
      "metadata": {
        "id": "5ab19d60-2a4b-4647-bdb6-f0f8c4aa2a27"
      },
      "outputs": [],
      "source": [
        "#Testing locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a077f1b-d9ef-4d82-b6a3-087e3b69d110",
      "metadata": {
        "id": "5a077f1b-d9ef-4d82-b6a3-087e3b69d110",
        "outputId": "1391ffce-c1ad-4678-b4f7-1c459765bdc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the Schrödinger equation?\n",
            "The Schrödinger equation is a mathematical expression that describes the behavior of a quantum system, such as a particle in a box. It is a special form of the Schröding\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Specify the path to the locally saved model\n",
        "local_model_path = \"./fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
        "\n",
        "# Test with a sample input\n",
        "input_text = \"What is the Schrödinger equation?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate a response\n",
        "output = model.generate(**inputs, max_length=50)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c6e6f4-12d1-4cc2-9369-c2144e73bf8d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "dfa19961b9d0481aa1aba6240aa53197",
            "4431c740547149f1ae5d0bdf8bf216a6",
            "26ee2ac382d74f0bae4549db46742820",
            "b71f3ef5f38043a691fc8bcaea8bad46",
            "2f9953cb779a42fc9c3c21a3e3060d4e",
            "6ebb8cd7ed2846ad84db1baaa49ceac1",
            "fef5e95e0c614f0780947b35a8241988",
            "a56d6123a81f4fd78a5256424900b266",
            "d4d650041a0d41d3af60d247d47e7d8c",
            "e7dcae04fbe240c4b178d52701ece340",
            "1871c0289f1f4aa2b0d71bdb7c25dffb"
          ]
        },
        "id": "02c6e6f4-12d1-4cc2-9369-c2144e73bf8d",
        "outputId": "5c867663-864c-4b9c-de5c-b36240c07c73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfa19961b9d0481aa1aba6240aa53197",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4431c740547149f1ae5d0bdf8bf216a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/3.70M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26ee2ac382d74f0bae4549db46742820",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b71f3ef5f38043a691fc8bcaea8bad46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/3.70M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f9953cb779a42fc9c3c21a3e3060d4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "optimizer.pt:   0%|          | 0.00/7.47M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ebb8cd7ed2846ad84db1baaa49ceac1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fef5e95e0c614f0780947b35a8241988",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "optimizer.pt:   0%|          | 0.00/7.47M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a56d6123a81f4fd78a5256424900b266",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4d650041a0d41d3af60d247d47e7d8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7dcae04fbe240c4b178d52701ece340",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1871c0289f1f4aa2b0d71bdb7c25dffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics/commit/18343dec31780151c0ff4afd9984e806f125dd07', commit_message='Upload folder using huggingface_hub', commit_description='', oid='18343dec31780151c0ff4afd9984e806f125dd07', pr_url=None, repo_url=RepoUrl('https://huggingface.co/akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics', endpoint='https://huggingface.co', repo_type='model', repo_id='akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Push to Hugging Face Hub\n",
        "trainer.push_to_hub(commit_message=\"Fine-tuned smolLM2-135M with LoRA on camel-ai/physics\")\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"./fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\",\n",
        "    repo_id=\"akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94116745-9bb6-4da6-80f5-c488e35e4810",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "980704f2f889430bad1a800e714cfc26",
            "fa2e02c55e0a45f09a87cd96fd7a5006"
          ]
        },
        "id": "94116745-9bb6-4da6-80f5-c488e35e4810",
        "outputId": "98a22b4c-d525-4b7f-c4a7-1ad809fd0f75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "980704f2f889430bad1a800e714cfc26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa2e02c55e0a45f09a87cd96fd7a5006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/3.70M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the Schrödinger equation?\n",
            "\n",
            "The Schrödinger equation is a mathematical expression for the energy levels of a quantum system, which is a mathematical model of the behavior of matter and energy. It is given by:\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Specify the Hugging Face model path (update with the correct repository path)\n",
        "model_path = \"akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\"\n",
        "\n",
        "# Load the model and tokenizer from the Hugging Face Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Test with a sample input\n",
        "input_text = \"What is the Schrödinger equation?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate a response\n",
        "output = model.generate(**inputs, max_length=50)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7be750d-b825-436e-9982-4ea093454906",
      "metadata": {
        "id": "a7be750d-b825-436e-9982-4ea093454906"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# # Clear cache\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Reset memory allocations and free up GPU memory\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3eb5046-4c83-46fa-b037-42505c35794f",
      "metadata": {
        "id": "c3eb5046-4c83-46fa-b037-42505c35794f",
        "outputId": "263751c4-1b1f-4d3d-bff3-267f9a420dc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "WARNING:lighteval.logging.hierarchical_logger:main: (0, Namespace(subcommand='accelerate', model_config_path=None, model_args='pretrained=HuggingFaceTB/SmolLM2-135M', max_samples=None, override_batch_size=16, job_id='', output_dir='/content/output', save_details=False, push_to_hub=False, push_to_tensorboard=False, public_run=False, results_org=None, use_chat_template=False, system_prompt=None, dataset_loading_processes=1, custom_tasks=None, tasks='leaderboard|mmlu:college_physics|0|0', cache_dir=None, num_fewshot_seeds=1)),  {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Test all gather {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Test gather tensor\n",
            "WARNING:lighteval.logging.hierarchical_logger:    gathered_tensor tensor([0], device='cuda:0'), should be [0]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.069977]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Model loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Tokenizer truncation and padding size set to the left side.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    We are not in a distributed setting. Setting model_parallel to False.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Model parallel was set to False, max memory set to None and device map to None\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Using Data Parallelism, putting model on device cuda\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.443294]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Tasks loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    \u001b[33mIf you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\u001b[0m\n",
            "WARNING:lighteval.logging.hierarchical_logger:    lighteval/mmlu college_physics\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Loading documents, and requests\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:01.000103]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Setting seeds and waiting for all processes {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    setting seed to 1234 for random and numpy\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000057]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Evaluation {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Evaluate on 1 tasks.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Running RequestType.LOGLIKELIHOOD requests\n",
            "0it [00:00, ?it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|██████▍                                      | 1/7 [00:00<00:01,  4.03it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 16.57it/s]\u001b[A\n",
            "1it [00:00,  2.36it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 42.23it/s]\u001b[A\n",
            "2it [00:00,  3.68it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 43.24it/s]\u001b[A\n",
            "3it [00:00,  4.51it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 43.77it/s]\u001b[A\n",
            "4it [00:00,  4.39it/s]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:01.053786]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Compiling results {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000254]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Cleaning up {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000023]\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "|               Task               |Version|Metric|Value |   |Stderr|\n",
            "|----------------------------------|------:|------|-----:|---|-----:|\n",
            "|all                               |       |acc   |0.2157|±  |0.0409|\n",
            "|leaderboard:mmlu:college_physics:0|      0|acc   |0.2157|±  |0.0409|\n",
            "\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Saving experiment tracker\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:02.597116]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/bin/lighteval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/__main__.py\", line 62, in cli_evaluate\n",
            "    main_accelerate(args)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/hierarchical_logger.py\", line 175, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/main_accelerate.py\", line 91, in main\n",
            "    pipeline.save_and_push_results()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/pipeline.py\", line 317, in save_and_push_results\n",
            "    self.evaluation_tracker.save()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 184, in save\n",
            "    self.save_results(date_id, results_dict)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 203, in save_results\n",
            "    self.fs.mkdirs(output_dir_results, exist_ok=True)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/spec.py\", line 1589, in mkdirs\n",
            "    return self.makedirs(path, exist_ok=exist_ok)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 52, in makedirs\n",
            "    os.makedirs(path, exist_ok=exist_ok)\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<frozen os>\", line 225, in makedirs\n",
            "PermissionError: [Errno 13] Permission denied: '/content'\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65621902-b452-4a04-9333-9ae109fcaaf4",
      "metadata": {
        "id": "65621902-b452-4a04-9333-9ae109fcaaf4",
        "outputId": "0da06d7f-cca9-431d-cc9f-4d7647a64d3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "WARNING:lighteval.logging.hierarchical_logger:main: (0, Namespace(subcommand='accelerate', model_config_path=None, model_args='pretrained=akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics', max_samples=None, override_batch_size=16, job_id='', output_dir='/content/output', save_details=False, push_to_hub=False, push_to_tensorboard=False, public_run=False, results_org=None, use_chat_template=False, system_prompt=None, dataset_loading_processes=1, custom_tasks=None, tasks='leaderboard|mmlu:college_physics|0|0', cache_dir=None, num_fewshot_seeds=1)),  {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Test all gather {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Test gather tensor\n",
            "WARNING:lighteval.logging.hierarchical_logger:    gathered_tensor tensor([0], device='cuda:0'), should be [0]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.077968]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Model loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Tokenizer truncation and padding size set to the left side.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    We are not in a distributed setting. Setting model_parallel to False.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Model parallel was set to False, max memory set to None and device map to None\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Using Data Parallelism, putting model on device cuda\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.822619]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Tasks loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    \u001b[33mIf you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\u001b[0m\n",
            "WARNING:lighteval.logging.hierarchical_logger:    lighteval/mmlu college_physics\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Loading documents, and requests\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.992033]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Setting seeds and waiting for all processes {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    setting seed to 1234 for random and numpy\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000060]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Evaluation {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Evaluate on 1 tasks.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Running RequestType.LOGLIKELIHOOD requests\n",
            "0it [00:00, ?it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|██████▍                                      | 1/7 [00:00<00:01,  4.08it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 16.21it/s]\u001b[A\n",
            "1it [00:00,  2.31it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 35.33it/s]\u001b[A\n",
            "2it [00:00,  3.39it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 35.50it/s]\u001b[A\n",
            "3it [00:00,  3.99it/s]\n",
            "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 37.12it/s]\u001b[A\n",
            "4it [00:01,  3.93it/s]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:01.122267]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Compiling results {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000263]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Cleaning up {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000022]\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "|               Task               |Version|Metric|Value |   |Stderr|\n",
            "|----------------------------------|------:|------|-----:|---|-----:|\n",
            "|all                               |       |acc   |0.2843|±  |0.0449|\n",
            "|leaderboard:mmlu:college_physics:0|      0|acc   |0.2843|±  |0.0449|\n",
            "\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Saving experiment tracker\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:03.045712]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/bin/lighteval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/__main__.py\", line 62, in cli_evaluate\n",
            "    main_accelerate(args)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/hierarchical_logger.py\", line 175, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/main_accelerate.py\", line 91, in main\n",
            "    pipeline.save_and_push_results()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/pipeline.py\", line 317, in save_and_push_results\n",
            "    self.evaluation_tracker.save()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 184, in save\n",
            "    self.save_results(date_id, results_dict)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 203, in save_results\n",
            "    self.fs.mkdirs(output_dir_results, exist_ok=True)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/spec.py\", line 1589, in mkdirs\n",
            "    return self.makedirs(path, exist_ok=exist_ok)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 52, in makedirs\n",
            "    os.makedirs(path, exist_ok=exist_ok)\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<frozen os>\", line 225, in makedirs\n",
            "PermissionError: [Errno 13] Permission denied: '/content'\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d1c570-fcac-4df2-8b09-42d24e9d7238",
      "metadata": {
        "id": "d0d1c570-fcac-4df2-8b09-42d24e9d7238",
        "outputId": "9bab0e4e-89e8-4f89-cd9c-dff2d33ef556"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "WARNING:lighteval.logging.hierarchical_logger:main: (0, Namespace(subcommand='accelerate', model_config_path=None, model_args='pretrained=HuggingFaceTB/SmolLM2-135M', max_samples=None, override_batch_size=16, job_id='', output_dir='/content/output', save_details=False, push_to_hub=False, push_to_tensorboard=False, public_run=False, results_org=None, use_chat_template=False, system_prompt=None, dataset_loading_processes=1, custom_tasks=None, tasks='leaderboard|mmlu:conceptual_physics|0|0', cache_dir=None, num_fewshot_seeds=1)),  {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Test all gather {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Test gather tensor\n",
            "WARNING:lighteval.logging.hierarchical_logger:    gathered_tensor tensor([0], device='cuda:0'), should be [0]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.081195]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Model loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Tokenizer truncation and padding size set to the left side.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    We are not in a distributed setting. Setting model_parallel to False.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Model parallel was set to False, max memory set to None and device map to None\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Using Data Parallelism, putting model on device cuda\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.529157]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Tasks loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    \u001b[33mIf you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\u001b[0m\n",
            "WARNING:lighteval.logging.hierarchical_logger:    lighteval/mmlu conceptual_physics\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Loading documents, and requests\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.908564]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Setting seeds and waiting for all processes {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    setting seed to 1234 for random and numpy\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000058]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Evaluation {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Evaluate on 1 tasks.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Running RequestType.LOGLIKELIHOOD requests\n",
            "0it [00:00, ?it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|██▉                                         | 1/15 [00:00<00:06,  2.03it/s]\u001b[A\n",
            " 40%|█████████████████▌                          | 6/15 [00:00<00:00, 12.27it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 18.15it/s]\u001b[A\n",
            "1it [00:00,  1.21it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|██████████████▋                             | 5/15 [00:00<00:00, 44.19it/s]\u001b[A\n",
            " 67%|████████████████████████████▋              | 10/15 [00:00<00:00, 44.15it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 44.45it/s]\u001b[A\n",
            "2it [00:01,  1.85it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|██████████████▋                             | 5/15 [00:00<00:00, 44.01it/s]\u001b[A\n",
            " 67%|████████████████████████████▋              | 10/15 [00:00<00:00, 43.36it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 28.70it/s]\u001b[A\n",
            "3it [00:01,  1.88it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|██████████████▋                             | 5/15 [00:00<00:00, 43.84it/s]\u001b[A\n",
            " 67%|████████████████████████████▋              | 10/15 [00:00<00:00, 43.38it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 43.41it/s]\u001b[A\n",
            "4it [00:02,  1.97it/s]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:02.318047]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Compiling results {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000432]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Cleaning up {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000025]\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "|                Task                 |Version|Metric|Value |   |Stderr|\n",
            "|-------------------------------------|------:|------|-----:|---|-----:|\n",
            "|all                                  |       |acc   |0.2681|±  | 0.029|\n",
            "|leaderboard:mmlu:conceptual_physics:0|      0|acc   |0.2681|±  | 0.029|\n",
            "\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Saving experiment tracker\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:03.871881]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/bin/lighteval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/__main__.py\", line 62, in cli_evaluate\n",
            "    main_accelerate(args)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/hierarchical_logger.py\", line 175, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/main_accelerate.py\", line 91, in main\n",
            "    pipeline.save_and_push_results()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/pipeline.py\", line 317, in save_and_push_results\n",
            "    self.evaluation_tracker.save()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 184, in save\n",
            "    self.save_results(date_id, results_dict)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 203, in save_results\n",
            "    self.fs.mkdirs(output_dir_results, exist_ok=True)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/spec.py\", line 1589, in mkdirs\n",
            "    return self.makedirs(path, exist_ok=exist_ok)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 52, in makedirs\n",
            "    os.makedirs(path, exist_ok=exist_ok)\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<frozen os>\", line 225, in makedirs\n",
            "PermissionError: [Errno 13] Permission denied: '/content'\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:conceptual_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir \"$OUTPUT_DIR\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e2131f-d03b-4150-8d52-f5a00f5d2bb3",
      "metadata": {
        "id": "04e2131f-d03b-4150-8d52-f5a00f5d2bb3",
        "outputId": "6acc5574-fd42-4d62-a852-b2631a790988"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "WARNING:lighteval.logging.hierarchical_logger:main: (0, Namespace(subcommand='accelerate', model_config_path=None, model_args='pretrained=akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics', max_samples=None, override_batch_size=16, job_id='', output_dir='/content/output', save_details=False, push_to_hub=False, push_to_tensorboard=False, public_run=False, results_org=None, use_chat_template=False, system_prompt=None, dataset_loading_processes=1, custom_tasks=None, tasks='leaderboard|mmlu:conceptual_physics|0|0', cache_dir=None, num_fewshot_seeds=1)),  {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Test all gather {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Test gather tensor\n",
            "WARNING:lighteval.logging.hierarchical_logger:    gathered_tensor tensor([0], device='cuda:0'), should be [0]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.080389]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Model loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Tokenizer truncation and padding size set to the left side.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    We are not in a distributed setting. Setting model_parallel to False.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Model parallel was set to False, max memory set to None and device map to None\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Using Data Parallelism, putting model on device cuda\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.827052]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Tasks loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    \u001b[33mIf you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\u001b[0m\n",
            "WARNING:lighteval.logging.hierarchical_logger:    lighteval/mmlu conceptual_physics\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Loading documents, and requests\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:01.128968]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Setting seeds and waiting for all processes {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    setting seed to 1234 for random and numpy\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000063]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Evaluation {\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Evaluate on 1 tasks.\n",
            "WARNING:lighteval.logging.hierarchical_logger:    Running RequestType.LOGLIKELIHOOD requests\n",
            "0it [00:00, ?it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|██▉                                         | 1/15 [00:00<00:03,  4.10it/s]\u001b[A\n",
            " 33%|██████████████▋                             | 5/15 [00:00<00:00, 16.54it/s]\u001b[A\n",
            " 60%|██████████████████████████▍                 | 9/15 [00:00<00:00, 23.15it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 23.27it/s]\u001b[A\n",
            "1it [00:00,  1.55it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27%|███████████▋                                | 4/15 [00:00<00:00, 37.68it/s]\u001b[A\n",
            " 53%|███████████████████████▍                    | 8/15 [00:00<00:00, 37.21it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 25.41it/s]\u001b[A\n",
            "2it [00:01,  1.63it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27%|███████████▋                                | 4/15 [00:00<00:00, 33.89it/s]\u001b[A\n",
            " 53%|███████████████████████▍                    | 8/15 [00:00<00:00, 34.81it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 35.38it/s]\u001b[A\n",
            "3it [00:01,  1.90it/s]\n",
            "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27%|███████████▋                                | 4/15 [00:00<00:00, 37.50it/s]\u001b[A\n",
            " 53%|███████████████████████▍                    | 8/15 [00:00<00:00, 36.80it/s]\u001b[A\n",
            "100%|███████████████████████████████████████████| 15/15 [00:00<00:00, 37.42it/s]\u001b[A\n",
            "4it [00:02,  1.94it/s]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:02.219990]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Compiling results {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000422]\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Cleaning up {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  } [0:00:00.000021]\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "|                Task                 |Version|Metric|Value |   |Stderr|\n",
            "|-------------------------------------|------:|------|-----:|---|-----:|\n",
            "|all                                  |       |acc   |0.2723|±  |0.0291|\n",
            "|leaderboard:mmlu:conceptual_physics:0|      0|acc   |0.2723|±  |0.0291|\n",
            "\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Saving experiment tracker\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "INFO:accelerate.accelerator:Deep copying the `Accelerator` object, note that this will point to the same original object.\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:04.291250]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/bin/lighteval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/__main__.py\", line 62, in cli_evaluate\n",
            "    main_accelerate(args)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/hierarchical_logger.py\", line 175, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/main_accelerate.py\", line 91, in main\n",
            "    pipeline.save_and_push_results()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/pipeline.py\", line 317, in save_and_push_results\n",
            "    self.evaluation_tracker.save()\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 184, in save\n",
            "    self.save_results(date_id, results_dict)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/lighteval/logging/evaluation_tracker.py\", line 203, in save_results\n",
            "    self.fs.mkdirs(output_dir_results, exist_ok=True)\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/spec.py\", line 1589, in mkdirs\n",
            "    return self.makedirs(path, exist_ok=exist_ok)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/avallala2023/anaconda3/envs/Thesis/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 52, in makedirs\n",
            "    os.makedirs(path, exist_ok=exist_ok)\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  File \"<frozen os>\", line 215, in makedirs\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<frozen os>\", line 225, in makedirs\n",
            "PermissionError: [Errno 13] Permission denied: '/content'\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "!lighteval accelerate \\\n",
        "    --model_args \"pretrained=$MODEL\" \\\n",
        "    --tasks \"leaderboard|mmlu:conceptual_physics|0|0\" \\\n",
        "    --override_batch_size 16 \\\n",
        "    --output_dir \"$OUTPUT_DIR\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Thesis",
      "language": "python",
      "name": "thesis"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}