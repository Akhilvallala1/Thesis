{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fa4e7-de05-4fe0-9690-7f4c339bac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Prompt the user for their Hugging Face token\n",
    "huggingface_token = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=huggingface_token)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cfd43-cd84-44bc-9268-09419b20599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "dataset_name = \"akhilfau/physics_decontaminated_2\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")  # Adjust the split if needed (e.g., \"test\" or \"validation\")\n",
    "\n",
    "# Print a sample record\n",
    "print(\"Sample record from the dataset:\")\n",
    "print(dataset[0])  # Prints the first record from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e93b1-ff80-414c-a21c-8beb6fff9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "dataset = load_dataset(\"akhilfau/physics_decontaminated_2\", split=\"train\")\n",
    "\n",
    "# Step 2: Load the Pretrained Model and Tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the padding token if not already set\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
    "\n",
    "# Step 3: Configure LoRA with PEFT\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters to confirm LoRA is applied\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 4: Preprocess the Dataset\n",
    "def preprocess_function(examples):\n",
    "    # Concatenate the problem and solution for causal LM\n",
    "    inputs = [f\"Problem: {problem}\\nSolution: {solution}\" for problem, solution in zip(examples[\"message_1\"], examples[\"message_2\"])]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Labels are the same as input_ids for causal LM\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Step 5: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_fine_tuned_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hugging Face Hub\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Step 6: Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Step 7: Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the Model and Tokenizer Locally\n",
    "model.save_pretrained(\"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\")\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "trainer.push_to_hub(commit_message=\"Fine-tuned smolLM2-135M with LoRA on camel-ai/physics\")\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\",\n",
    "    repo_id=\"akhilfau/fine-tuned-smolLM2-135M-with-LoRA-on-camel-ai-physics\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be750d-b825-436e-9982-4ea093454906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Clear cache\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Reset memory allocations and free up GPU memory\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef6be7-d605-4e77-ab2e-401b27a5e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# for i in range(torch.cuda.device_count()):     \n",
    "#     print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6e6f4-12d1-4cc2-9369-c2144e73bf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
